{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiDNe42BX1v3eVtyfOT+o/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivek2222/AI-Launchpad/blob/main/Day_2_Assignment_1_Implement_the_Transformer_Model_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 6-1 Transformers Model: https://github.com/toni-ramchandani/AIMasterClassTTT/blob/main/Section6-1_Transformers.ipynb"
      ],
      "metadata": {
        "id": "W249Vr37RAKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Set Up Your Google Colab Environment**"
      ],
      "metadata": {
        "id": "-W1KFX-_SMcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries: These commands install PyTorch, which we'll use to implement the Transformer model.\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POBQCZTQSPKS",
        "outputId": "3c60e539-6834-4d06-c8f6-cddb9358335e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Implement the Transformer Model**"
      ],
      "metadata": {
        "id": "FQ0JFciTStA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "s1fYOSNfSw9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the Scaled Dot-Product Attention:\n",
        "Scaled Dot-Product Attention is a core component of the Transformer architecture used in natural language processing and computer vision. It computes the attention weights for each element in a sequence relative to all others. It is \"scaled\" to prevent extremely large dot-product values when the dimensions of input vectors are large.\n",
        "\n",
        "#Implementing Self-Attention Mechanism\n",
        "Below is the implementation of the self-attention mechanism, which incorporates Scaled Dot-Product Attention:"
      ],
      "metadata": {
        "id": "AFoZrO2JXFJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ScaledDotProductAttention class, which is a type of attention mechanism used in Transformer models\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k  # d_k is the dimensionality of the keys and queries, used for scaling the dot product\n",
        "\n",
        "    # The forward method defines how the input data moves through this layer\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Compute the dot product between the query and the transpose of the key\n",
        "        # The transpose operation swaps the last two dimensions of the key\n",
        "        # This dot product gives us a score matrix that represents the similarity between queries and keys\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # If a mask is provided, apply it to the scores\n",
        "        # This is usually done to ignore certain positions in the input (e.g., padding tokens)\n",
        "        # The masked positions are filled with a large negative value (-1e9) so that their softmax result is close to zero\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply the softmax function to the scores to obtain attention weights\n",
        "        # Softmax is applied along the last dimension to ensure the weights sum up to 1\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Multiply the attention weights with the value vectors\n",
        "        # This step generates the output by weighting the value vectors according to the attention weights\n",
        "        output = torch.matmul(attention, value)\n",
        "\n",
        "        # Return the output and the attention weights\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "qmhbFmJfXE5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary:**\n",
        "\n",
        "The provided code defines a **ScaledDotProductAttention** class, which implements the core mechanism of attention in Transformer models. The goal is to calculate **attention scores** that measure the similarity between a set of queries and keys, normalize these scores using softmax to form **attention weights**, and use these weights to combine the corresponding values into a final output.\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1. **Compute Scores:**  \n",
        "   Calculate similarity between each query and key using a scaled dot product:\n",
        "![Screenshot 2024-12-07 235901.png](data:image/png;base64,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)\n",
        "2. **Apply Mask (Optional):**  \n",
        "   Mask specific positions (e.g., padding tokens) by assigning very negative values to their scores, so their influence is near zero after softmax.\n",
        "\n",
        "3. **Calculate Attention Weights:**  \n",
        "   Use softmax to normalize the scores along the last dimension, converting them into probabilities.\n",
        "\n",
        "4. **Generate Output:**  \n",
        "   Multiply the attention weights by the corresponding values to get a weighted sum.\n",
        "\n",
        "5. **Return:**  \n",
        "   The final output and the attention weights are returned.\n",
        "\n",
        "---\n",
        "\n",
        "### **Analogy: A Search Engine Example**\n",
        "\n",
        "Imagine you are searching for a specific topic on a search engine:\n",
        "\n",
        "1. **Query (You):**  \n",
        "   You type a query (e.g., \"best pizza places\"). This represents the **query vector**.\n",
        "\n",
        "2. **Key (Web Pages):**  \n",
        "   The search engine has metadata (keywords) for every web page. These are the **key vectors**.\n",
        "\n",
        "3. **Value (Web Pages' Content):**  \n",
        "   The actual content of the web pages is the **value vector**.\n",
        "\n",
        "4. **Relevance Scores:**  \n",
        "   The search engine compares your query against all keys (metadata) using a similarity measure (e.g., dot product). It scales these scores to balance large queries.\n",
        "\n",
        "5. **Normalization (Softmax):**  \n",
        "   The search engine ranks the results by normalizing the relevance scores into probabilities.\n",
        "\n",
        "6. **Final Output:**  \n",
        "   You receive the most relevant web pages, weighted by their relevance to your query. This corresponds to the **attention mechanism combining values (web pages)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway:**\n",
        "This class lets a Transformer model focus on the most relevant parts of the input by calculating **attention weights** based on similarity, normalizing them, and generating a context-aware output."
      ],
      "metadata": {
        "id": "4c_vKBOYX7lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the Multi-Head Attention Layer:\n",
        "\n",
        "Multi-Head Attention (MHA) is a mechanism that enhances the self-attention process in Transformer models by allowing the model to focus on different parts of the input sequence simultaneously. It achieves this by creating multiple \"attention heads,\" where each head independently learns attention weights and computes context vectors. These outputs are then combined to produce the final result.\n",
        "\n",
        "#How Does Multi-Head Attention Work?\n",
        "**Input Projections:**\n",
        "The input (e.g., word embeddings) is linearly transformed into query (Q), key (K), and value (V) vectors for each head. These projections are learned independently for every attention head.\n",
        "\n",
        "**Scaled Dot-Product Attention:**\n",
        "Each head performs scaled dot-product attention on its projected ùëÑ,K,and V, allowing the model to focus on different relationships or patterns in the input.\n",
        "\n",
        "**Concatenation:**\n",
        "The outputs of all attention heads are concatenated.\n",
        "\n",
        "**Final Projection:**\n",
        "The concatenated output is passed through a final linear layer to produce the output of the multi-head attention mechanism."
      ],
      "metadata": {
        "id": "LuesotexZDmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Define the MultiHeadAttention class, which is a core component of the Transformer model\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads  # The number of attention heads\n",
        "        self.d_k = d_model // num_heads  # The dimension of each head (d_model divided by num_heads)\n",
        "\n",
        "        # Linear layers to project the input query, key, and value vectors into the required dimensions\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linear layer to project the concatenated output of all heads back into the original d_model dimension\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # The forward method defines how the input data moves through this layer\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)  # Get the batch size from the query input\n",
        "\n",
        "        # Project the query, key, and value inputs into multiple heads\n",
        "        # Each projection is reshaped to [batch_size, num_heads, sequence_length, d_k]\n",
        "        query = self.query_linear(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = self.key_linear(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = self.value_linear(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention for each head\n",
        "        # The attention function returns the attention-weighted values\n",
        "        attention, _ = ScaledDotProductAttention(self.d_k)(query, key, value, mask)\n",
        "\n",
        "        # Transpose and reshape the attention output back to [batch_size, sequence_length, d_model]\n",
        "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "\n",
        "        # Apply the final linear transformation to combine the heads' outputs\n",
        "        output = self.out_linear(attention)\n",
        "        return output  # Return the final output of the multi-head attention mechanism"
      ],
      "metadata": {
        "id": "glnUtldUaC6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Code**\n",
        "\n",
        "This code defines a **Multi-Head Attention (MHA)** module, a fundamental part of Transformer models like GPT and BERT. It allows the model to focus on multiple parts of the input sequence simultaneously, improving its ability to understand context and relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### **Analogy: Multi-Head Attention as a Team of Experts**\n",
        "\n",
        "Imagine you‚Äôre reading a book with a team of experts:\n",
        "1. **Team Members (Heads):**  \n",
        "   Each expert focuses on a specific aspect of the text (e.g., grammar, plot, or character development).\n",
        "\n",
        "2. **Dividing the Work (Splitting into Heads):**  \n",
        "   The text is given to each expert, who reviews it based on their expertise.\n",
        "\n",
        "3. **Individual Analysis (Attention in Heads):**  \n",
        "   Each expert evaluates how important different parts of the text are (queries and keys) and extracts the relevant information (values).\n",
        "\n",
        "4. **Combining Results:**  \n",
        "   All the experts' insights are brought together, summarized, and presented as a cohesive understanding of the text.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "- **Purpose:**  \n",
        "  Multi-Head Attention enables a Transformer model to focus on multiple relationships within the input sequence simultaneously.\n",
        "\n",
        "- **Steps:**  \n",
        "  1. Split input embeddings into multiple attention heads.\n",
        "  2. Apply scaled dot-product attention in parallel for each head.\n",
        "  3. Concatenate and merge the outputs of all heads.\n",
        "\n",
        "- **Key Benefits:**\n",
        "  - Diverse Focus:\n",
        "    Each head captures different patterns or relationships.\n",
        "  - Improved Representation:\n",
        "    Allows the model to process input sequences more contextually.\n",
        "  -Scalability:\n",
        "    Works effectively for long sequences by breaking the attention mechanism into smaller, parallel parts.\n",
        "\n",
        "In short, MHA is a powerful mechanism for helping models like GPT understand text better by looking at multiple aspects of the input at the same time.\n",
        "\n"
      ],
      "metadata": {
        "id": "UXc36D7IaNQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implement Positional Encoding:\n",
        "\n",
        "Positional encoding is crucial in Transformer models as they lack inherent sequential information. Unlike RNNs, which process tokens in sequence, Transformers process all tokens simultaneously. Positional encoding provides the model with information about the order of tokens in a sequence.\n",
        "\n",
        "**Add positional encoding to handle the order of tokens in the sequence:**"
      ],
      "metadata": {
        "id": "E_2_IVIecR9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PositionalEncoding class, which is used to inject information about the relative or absolute position of tokens in a sequence.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Initialize an empty encoding matrix with shape [max_len, d_model]\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Create a tensor with shape [max_len, 1], where each element represents the position in the sequence\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Calculate the division term, which is based on the position and model dimension\n",
        "        # The division term varies across dimensions and is used to scale the position\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices (starting at 0) in the encoding matrix\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cosine to odd indices (starting at 1) in the encoding matrix\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add an extra dimension at the start of the encoding tensor to match batch dimensions\n",
        "        # The resulting shape is [1, max_len, d_model]\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    # Forward method that adds the positional encoding to the input tensor x\n",
        "    def forward(self, x):\n",
        "        # The encoding tensor is added to the input x, which is expected to have a shape of [batch_size, sequence_length, d_model]\n",
        "        # The encoding is sliced to match the sequence length of x and moved to the same device as x\n",
        "        return x + self.encoding[:, :x.size(1), :].to(x.device)"
      ],
      "metadata": {
        "id": "nHBRK-rNc9lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Key Points\n",
        "1. Purpose:\n",
        "Positional encoding injects sequential information into the model.\n",
        "\n",
        "2. Application:\n",
        "It is added to input embeddings before passing them to attention layers.\n",
        "\n",
        "3. Interpretation:\n",
        "Each token in the sequence gets a unique \"position signal\" that helps the model understand the order.\n",
        "\n",
        "With positional encoding, Transformers can process sequences while respecting token order, enabling them to handle tasks like language modeling or sequence translation effectively."
      ],
      "metadata": {
        "id": "924IeXMDdGlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analogy**\n",
        "Think of positional encoding as assigning \"coordinates\" to each token in a sequence:\n",
        "- Imagine a parade where each marcher has a specific position.\n",
        "- Each marcher's uniform has a unique pattern based on their position.\n",
        "- These patterns help observers (the Transformer model) distinguish and understand the roles and relationships of the marchers (tokens).\n",
        "\n",
        "### **Summary**\n",
        "- **Purpose:** Adds information about the position of tokens in a sequence to their embeddings.  \n",
        "- **Why Sine and Cosine?**  \n",
        "  - They provide a continuous and unique signal for each position.  \n",
        "  - They make relative position differences easy to compute.  \n",
        "- **How it Helps:** Positional encodings allow Transformers to model the sequence order, enabling them to process tasks like text understanding, translation, and more.  "
      ],
      "metadata": {
        "id": "yKlGuAnTdjXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation of the EncoderLayer Code\n",
        "This EncoderLayer class represents one layer of a Transformer encoder. Each layer combines two main components: a self-attention mechanism and a feedforward neural network, with residual connections and normalization applied to improve learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "t5IBwLe-d0sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the EncoderLayer class, which is a single layer of the Transformer encoder\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Initialize the MultiHeadAttention mechanism\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Define the feedforward network as a sequence of layers\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),  # Linear transformation from d_model to d_ff dimensions\n",
        "            nn.ReLU(),  # Apply ReLU activation function\n",
        "            nn.Linear(d_ff, d_model)  # Linear transformation back to d_model dimensions\n",
        "        )\n",
        "\n",
        "        # Layer normalization applied after the self-attention sub-layer\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Layer normalization applied after the feedforward sub-layer\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # The forward method defines the flow of data through this layer\n",
        "    def forward(self, x, mask=None):\n",
        "        # Apply self-attention mechanism\n",
        "        # The input x is passed as query, key, and value, which is typical for self-attention\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "\n",
        "        # Add the attention output to the input (residual connection) and apply layer normalization\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Pass the normalized output through the feedforward network\n",
        "        ff_output = self.ff(x)\n",
        "\n",
        "        # Add the feedforward output to the input (residual connection) and apply layer normalization\n",
        "        x = self.layer_norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        # Return the final output of this encoder layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "t_B9XbTgeFdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analogy**\n",
        "Think of this as a two-step refinement process for understanding a paragraph:\n",
        "- **Self-Attention:** Like reviewing each sentence in the paragraph in the context of the others to grasp the bigger picture.\n",
        "- **Feedforward Network:** Adds deeper meaning to each sentence based on the refined understanding.  \n",
        "The residual connections ensure the model doesn‚Äôt lose any original context during these refinements.\n",
        "\n",
        "### **Summary**\n",
        "- **Purpose:** Processes a sequence of tokens by enriching their embeddings with context and additional layers of meaning.  \n",
        "- **Key Features:**  \n",
        "  - **Self-Attention:** Helps each token focus on other relevant tokens.  \n",
        "  - **Feedforward Network:** Introduces depth to the representation.  \n",
        "  - **LayerNorm and Residual Connections:** Stabilize training and ensure no information is lost.  \n",
        "- **Output:** Enhanced token representations, ready for deeper layers of the Transformer.  "
      ],
      "metadata": {
        "id": "3NESmgVleF1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Explanation of the DecoderLayer Code**\n",
        "\n",
        "The `DecoderLayer` class represents one layer of a Transformer decoder. It combines three main components:\n",
        "\n",
        "1. **Self-Attention Mechanism**:  \n",
        "   The first attention mechanism focuses on understanding the relationships within the target sequence (the sequence being decoded). Each token in the target sequence attends to other tokens within the same sequence, helping the model understand the context of each token in relation to the others.\n",
        "\n",
        "2. **Cross-Attention Mechanism**:  \n",
        "   The second attention mechanism is designed to allow the decoder to attend to the encoder's output. This is crucial for tasks like machine translation, where the decoder must generate a sequence based on the information in the source sequence (processed by the encoder).\n",
        "\n",
        "3. **Feedforward Neural Network**:  \n",
        "   After both attention mechanisms process the input, the output is passed through a feedforward neural network. This network adds depth and complexity to the learned representations, making them more expressive and capable of capturing intricate patterns.\n",
        "\n",
        "4. **Residual Connections & Layer Normalization**:  \n",
        "   To stabilize training and ensure the model does not lose important information, residual connections are used. These allow the input to bypass the layers and be added directly to the output. Layer normalization is applied after each sub-layer (self-attention, cross-attention, and feedforward) to ensure consistent scaling of the activations and improve model convergence.\n",
        "\n",
        "### **Summary**:\n",
        "The `DecoderLayer` processes the target sequence while leveraging information from the source sequence. The combination of self-attention, cross-attention, and feedforward neural networks with normalization and residual connections ensures that the model can effectively generate sequences by understanding both the input and its own outputs."
      ],
      "metadata": {
        "id": "ntx_b8UteK2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DecoderLayer class, which is a single layer of the Transformer decoder\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Self-attention mechanism for the target sequence (decoder input)\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Cross-attention mechanism that attends to the encoder output\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Feedforward network as a sequence of layers\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),  # Linear transformation from d_model to d_ff dimensions\n",
        "            nn.ReLU(),  # Apply ReLU activation function\n",
        "            nn.Linear(d_ff, d_model)  # Linear transformation back to d_model dimensions\n",
        "        )\n",
        "\n",
        "        # Layer normalization applied after self-attention sub-layer\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Layer normalization applied after cross-attention sub-layer\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Layer normalization applied after the feedforward sub-layer\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # The forward method defines the flow of data through this layer\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Apply self-attention mechanism on the decoder input (target sequence)\n",
        "        # The input x is passed as query, key, and value, similar to the encoder's self-attention\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "\n",
        "        # Add the attention output to the input (residual connection) and apply layer normalization\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Apply cross-attention mechanism, attending to the encoder output (source sequence)\n",
        "        # The decoder input x is the query, and the encoder output (enc_output) is the key and value\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "\n",
        "        # Add the cross-attention output to the input and apply layer normalization\n",
        "        x = self.layer_norm2(x + self.dropout(attn_output))\n",
        "\n",
        "        # Pass the normalized output through the feedforward network\n",
        "        ff_output = self.ff(x)\n",
        "\n",
        "        # Add the feedforward output to the input (residual connection) and apply layer normalization\n",
        "        x = self.layer_norm3(x + self.dropout(ff_output))\n",
        "\n",
        "        # Return the final output of this decoder layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "Sh9XX4MseTqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analogy**\n",
        "Think of the **DecoderLayer** as a skilled translator in a language translation task:\n",
        "\n",
        "1. **Self-Attention** (in the decoder):  \n",
        "   Like a translator reading the sentence they are translating and referring back to different parts of it, trying to understand the full context of each word within that sentence.\n",
        "\n",
        "2. **Cross-Attention** (with encoder output):  \n",
        "   Once the translator understands the current sentence, they reference the source sentence (encoded by the encoder) to ensure their translation matches the meaning conveyed in the original language.\n",
        "\n",
        "3. **Feedforward Network**:  \n",
        "   The translator refines their translation after considering the context from the source sentence and the target sentence. They may adjust the wording or syntax for better fluency.\n",
        "\n",
        "4. **Residual Connections & LayerNorm**:  \n",
        "   Ensure the original meaning (context) is retained during translation, and adjustments (like word choices or sentence structure) are applied without losing the original sentence‚Äôs integrity.\n",
        "\n",
        "### **Summary**\n",
        "- **Purpose**: Processes the target sequence while also considering the encoder's output, which helps in generating translated sequences (or sequences in general).\n",
        "  \n",
        "- **Key Features**:\n",
        "  - **Self-Attention**: Helps the decoder focus on relevant parts of the target sequence.\n",
        "  - **Cross-Attention**: Allows the decoder to attend to the encoder‚Äôs output, capturing the relationship between source and target sequences.\n",
        "  - **Feedforward Network**: Refines the decoded output to enhance the representation.\n",
        "  - **Residual Connections & LayerNorm**: Stabilizes training and ensures the decoder‚Äôs output remains rich in context.\n",
        "\n",
        "- **Output**: Refined sequence representation, incorporating information from both the target (self-attention) and the source (cross-attention) sequences, ready to be processed further in the decoder stack."
      ],
      "metadata": {
        "id": "sfygqMv-eXf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Transformer Model\n",
        "\n",
        "### **Explanation of the Transformer Model Code**\n",
        "\n",
        "The `Transformer` class represents the entire Transformer architecture, which is a deep learning model designed for sequence-to-sequence tasks like translation, text generation, etc. It consists of multiple components including embeddings, positional encodings, encoder and decoder layers, and a final output layer.\n",
        "\n",
        "#### **Key Components**:\n",
        "\n",
        "1. **Encoder Embedding Layer**:\n",
        "   - The source input sequence (for example, in machine translation, this could be the sentence in the source language) is first passed through an embedding layer.\n",
        "   - This layer converts each token in the input sequence into a dense vector representation of size `d_model`.\n",
        "\n",
        "2. **Decoder Embedding Layer**:\n",
        "   - Similarly, the target sequence (e.g., the sentence in the target language) is passed through a separate embedding layer.\n",
        "   - It also converts tokens in the target sequence into embeddings of size `d_model`.\n",
        "\n",
        "3. **Positional Encoding**:\n",
        "   - Since the Transformer doesn't have a built-in mechanism to understand the order of tokens, we add **positional encoding** to the token embeddings to provide information about the position of tokens within the sequence.\n",
        "   - This is done using a `PositionalEncoding` class that applies sine and cosine functions to create a unique encoding for each position.\n",
        "\n",
        "4. **Encoder Layers**:\n",
        "   - The model contains a stack of **encoder layers**. Each encoder layer includes a self-attention mechanism, followed by a feedforward network.\n",
        "   - The input sequence is processed through each encoder layer, refining the representation at each step.\n",
        "\n",
        "5. **Decoder Layers**:\n",
        "   - The model also contains a stack of **decoder layers**. Each decoder layer has:\n",
        "     - **Self-attention**: Helps each token in the target sequence focus on other tokens in the target sequence.\n",
        "     - **Cross-attention**: Allows the decoder to focus on the encoder's output (the source sequence), helping it generate an output sequence based on the input sequence.\n",
        "     - **Feedforward Network**: Introduces further complexity and depth to the decoder's representations.\n",
        "\n",
        "6. **Final Linear Layer**:\n",
        "   - After passing through all the decoder layers, the final output of the decoder is passed through a **linear layer**.\n",
        "   - This layer maps the decoder's output into a vector of size equal to the vocabulary size (the output space).\n",
        "   - This output is then used for generating predictions (like the next token in a sequence) and is usually followed by a **softmax** layer during training to convert the output into probabilities.\n",
        "\n",
        "#### **How It Works (Flow)**:\n",
        "- **Input Sequence (Source)**: The input sequence is embedded and positional encoding is added. It is passed through multiple encoder layers to get a refined representation.\n",
        "  \n",
        "- **Output Sequence (Target)**: The target sequence is also embedded, positional encoding is added, and then it passes through multiple decoder layers. The decoder attends to both the target sequence (via self-attention) and the encoder's output (via cross-attention).\n",
        "\n",
        "- **Final Prediction**: The final output of the decoder layers is passed through a linear layer to get the prediction, typically representing the next token in the sequence.\n",
        "\n",
        "#### **Summary**:\n",
        "The `Transformer` class is the complete architecture that implements the core functionality of the Transformer model. It uses embeddings, positional encodings, a stack of encoder and decoder layers, and a final linear layer to generate sequence-to-sequence outputs. The self-attention and cross-attention mechanisms, along with feedforward networks and normalization, ensure that the model can efficiently capture complex relationships in sequence data, making it powerful for tasks like machine translation, text generation, and more."
      ],
      "metadata": {
        "id": "p9V5jbpWoapX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer class, which represents the complete Transformer model architecture\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, input_vocab_size, output_vocab_size, max_len=5000, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Embedding layer for the source input sequence (encoder input)\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "\n",
        "        # Embedding layer for the target input sequence (decoder input)\n",
        "        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding to add positional information to the input embeddings\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Stack of encoder layers, each composed of self-attention and feedforward sub-layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)])\n",
        "\n",
        "        # Stack of decoder layers, each composed of self-attention, cross-attention, and feedforward sub-layers\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)])\n",
        "\n",
        "        # Final linear layer that maps the decoder output to the output vocabulary size\n",
        "        self.final_linear = nn.Linear(d_model, output_vocab_size)\n",
        "\n",
        "    # The forward method defines how the data flows through the Transformer model\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # Embed the source input sequence (src) and scale the embeddings by the square root of the model dimension\n",
        "        src = self.encoder_embedding(src) * torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "\n",
        "        # Add positional encodings to the embedded source sequence\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        # Pass the source sequence through each encoder layer in the stack\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # Embed the target input sequence (tgt) and scale the embeddings by the square root of the model dimension\n",
        "        tgt = self.decoder_embedding(tgt) * torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "\n",
        "        # Add positional encodings to the embedded target sequence\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "\n",
        "        # Pass the target sequence through each decoder layer in the stack\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, src, tgt_mask, src_mask)\n",
        "\n",
        "        # Pass the output of the final decoder layer through the final linear layer\n",
        "        # This maps the output to the vocabulary space, producing a distribution over the output vocabulary\n",
        "        output = self.final_linear(tgt)\n",
        "\n",
        "        # Return the final output, which is typically passed to a softmax layer during training\n",
        "        return output"
      ],
      "metadata": {
        "id": "qam_pJlpogqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analogy**:\n",
        "Think of the Transformer model as a **translator** working on a **two-step process**:\n",
        "\n",
        "1. **Encoder (Source Language Understanding)**:\n",
        "   - Imagine you're trying to understand a sentence in a foreign language (e.g., French). The **encoder** is like a **linguist** who reads the sentence, processes it word by word, and identifies the relationships between the words to understand their meaning. However, the linguist also knows the order of the words (positional encoding) and can focus on relevant parts of the sentence (self-attention).\n",
        "   \n",
        "2. **Decoder (Target Language Generation)**:\n",
        "   - Once the linguist understands the meaning, they start translating the sentence into the target language (e.g., English). The **decoder** is like the **translator** who, while translating each word, looks at both the context of the sentence in the target language and the original sentence (via cross-attention) to ensure the translation is accurate. The translator continuously refines the translation (feedforward network) and ensures no part of the meaning is lost.\n",
        "\n",
        "### **Summary**:\n",
        "The Transformer model is a sophisticated architecture designed for tasks like translation or text generation. It works by processing input sequences through an **encoder** and output sequences through a **decoder**. Key components include:\n",
        "\n",
        "- **Embedding Layers**: Convert words into dense vectors.\n",
        "- **Positional Encoding**: Adds information about the order of words in the sequence.\n",
        "- **Self-Attention**: Allows each word to focus on other relevant words in the sequence.\n",
        "- **Cross-Attention**: Lets the decoder attend to the encoder's output for context.\n",
        "- **Feedforward Networks**: Introduce additional complexity to refine representations.\n",
        "- **Final Linear Layer**: Maps the decoder output to the target vocabulary.\n",
        "\n",
        "In essence, the Transformer model excels at handling sequential data by allowing flexible attention mechanisms, enabling it to process long-range dependencies and generate accurate outputs for complex tasks."
      ],
      "metadata": {
        "id": "emCGzaB_olDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Example Usage:\n",
        "\n",
        "In this example, we demonstrate how to test the Transformer model using dummy data. The goal is to verify that the model works with random input and output sequences, and ensure the shapes of the output tensor are as expected. Here's an explanation of each part:"
      ],
      "metadata": {
        "id": "boCsd7mponTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the vocabulary sizes for the source and target languages\n",
        "input_vocab_size = 10000  # Source vocabulary size\n",
        "output_vocab_size = 10000  # Target vocabulary size\n",
        "\n",
        "# Set the dimensionality of the model, which determines the size of the embeddings and the model's internal representations\n",
        "d_model = 512  # Dimensionality of the embeddings and model\n",
        "\n",
        "# Define the number of attention heads in the multi-head attention mechanism\n",
        "num_heads = 8  # Number of attention heads\n",
        "\n",
        "# Set the number of layers in the encoder and decoder stacks\n",
        "num_encoder_layers = 6  # Number of encoder layers\n",
        "num_decoder_layers = 6  # Number of decoder layers\n",
        "\n",
        "# Define the dimensionality of the feedforward network within each layer\n",
        "d_ff = 2048  # Dimensionality of the feedforward network\n",
        "\n",
        "# Set the maximum length for the input and output sequences\n",
        "max_len = 100  # Maximum length of the input and output sequences\n",
        "\n",
        "# Instantiate the Transformer model with the specified parameters\n",
        "model = Transformer(d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, input_vocab_size, output_vocab_size, max_len)\n",
        "\n",
        "# Generate a batch of random source sentences\n",
        "# Each sentence has 100 tokens, and there are 32 sentences in the batch\n",
        "src = torch.randint(0, input_vocab_size, (32, 100))  # Source sentences (randomly generated)\n",
        "\n",
        "# Generate a batch of random target sentences\n",
        "# Each sentence has 100 tokens, and there are 32 sentences in the batch\n",
        "tgt = torch.randint(0, output_vocab_size, (32, 100))  # Target sentences (randomly generated)\n",
        "\n",
        "# Pass the source and target sentences through the Transformer model\n",
        "output = model(src, tgt)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "# Expected shape: [32, 100, 10000], corresponding to [batch size, sequence length, output vocab size]\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltcxMdjrpXem",
        "outputId": "60d90ce7-8299-4176-beef-1dd7c3148332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analogy**:\n",
        "Imagine you're a translator at a conference, working with a group of people who speak different languages. You receive sentences from one language (the source), and you need to translate them into another language (the target).\n",
        "\n",
        "1. **The Transformer Model**: Think of it as a team of advanced translators.\n",
        "   - The **encoder** reads and understands the source language sentences (like you reading the input text in the source language).\n",
        "   - The **decoder** then uses the information from the encoder and translates it into the target language (like you generating a translated sentence based on the source).\n",
        "   - The **attention mechanism** allows the team to focus on the most relevant words in the source sentence while translating, just as you might emphasize key parts of the source text to ensure an accurate translation.\n",
        "\n",
        "2. **Training the Model**: Before being able to translate perfectly, the team (model) practices with many examples, learning how to align the source and target languages by understanding context, word relationships, and sequence.\n",
        "\n",
        "### **Summary**:\n",
        "- **Purpose**: The Transformer model handles the task of translating or transforming sequences (like sentences in language translation). It processes input (source) and generates output (target) while considering the context and relationships within the sequence.\n",
        "- **Core Components**:\n",
        "  - **Encoder**: Understands the input sequence (source language).\n",
        "  - **Decoder**: Translates the encoder‚Äôs understanding into the output sequence (target language).\n",
        "  - **Self-Attention**: Focuses on different parts of the sequence to understand relationships between words.\n",
        "  - **Feedforward Layers**: Add additional depth to the representation of the sequence.\n",
        "  - **Positional Encoding**: Ensures the model knows the order of words in a sentence (sequence order).\n",
        "- **Output**: The model generates a prediction for each token in the sequence, mapped to the vocabulary space. This is typically passed through a softmax layer to obtain probabilities for each possible token in the target sequence."
      ],
      "metadata": {
        "id": "gYXbmNDapiCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Output**:\n",
        "The output of the Transformer model is a tensor of shape `[batch_size, sequence_length, output_vocab_size]`. For each position in the sequence, this tensor represents a probability distribution over the entire output vocabulary. This means that for each token in the target sequence, the model predicts the likelihood of every word in the target vocabulary, and the word with the highest probability is chosen as the predicted word.\n",
        "\n",
        "### **Step 3: Running the Model in Google Colab**\n",
        "\n",
        "1. **Execute the Code**:\n",
        "   - You can run each cell in your Google Colab notebook to build and test the Transformer model. This will allow you to evaluate how the model performs on random input and target sentences.\n",
        "\n",
        "2. **Utilize GPU**:\n",
        "   - Google Colab provides free access to GPUs, which can significantly speed up the training and evaluation of machine learning models.\n",
        "   - To enable GPU:\n",
        "     1. Go to **Runtime** in the top menu.\n",
        "     2. Select **Change runtime type**.\n",
        "     3. From the **Hardware accelerator** dropdown, choose **GPU**.\n",
        "     4. Save and continue.\n",
        "\n",
        "3. **Save and Share Your Notebook**:\n",
        "   - After running the notebook and testing the Transformer model, you can save your work and share it with others.\n",
        "     - To share the notebook, click **Share** in the top-right corner of the Colab interface.\n",
        "     - You can also download the notebook as a `.ipynb` file or directly export it to **GitHub**.\n",
        "\n",
        "### **Extending the Model**:\n",
        "This implementation is a solid starting point for exploring Transformer models. However, you can extend it by:\n",
        "- Adding **training loops** to train the model on specific datasets (like text translation or other sequence tasks).\n",
        "- Pre-processing data (e.g., tokenizing text data, padding sequences).\n",
        "- Fine-tuning the model on specific tasks such as language translation or text generation.\n",
        "- Experimenting with different **hyperparameters** (e.g., number of layers, attention heads, model size).\n",
        "\n",
        "This flexibility makes Transformers powerful for a variety of tasks in NLP (Natural Language Processing) and beyond."
      ],
      "metadata": {
        "id": "nyhtlUaspsOI"
      }
    }
  ]
}